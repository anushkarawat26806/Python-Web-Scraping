# Python-Web-Scraping
Beautiful Soup is a Python Library that is utilized to retrieve data out of HTML, XML, and alternate markup languages. It converts the web-page into a tree containing tags, elements, attributes, and values. Beautiful Soup can be utilized if you, for example, plan on visiting an article but do not plan on reading it in its entirety. In a situation like this, you can utilize Beautiful Soup to extract the particular information and attributes that are most relevant to what you are searching for. 
For this project, another thing I imported is Requests. Requests is a Python library used to easily make HTTP requests. Essentially, The main function of the Requests library is that it allow you to make use of the HTTP within your program by making it more readable to the human eye.

For my midterm project, I imported BeautifulSoup,Requests, and lxml to scrape information off of a Wikipedia Page - entired COVID 19 Pandemic. The information I wanted to scrape included the title of the page, all of the paragraph tags, searching for specific key words in the text, and extrracting links present in the Wikipedia page with its respective headers.

PACKAGES NEEDED TO RUN MY CODE:
1. Import requests by typing this into your terminal: python -m pip install requests
2. To get BeautifulSoup, type in pip install beautifulsoup4 into your terminal.
3. To install lxml, type in pip install lxml

CODE:
My code aims to review the Wikipedia article - titled COVID 19 Pandemic - and parse it's subject while returning important key information. While developing this project, I was mainly thinking how I could use BeautifulSoup to spit out important information. For example, sometimes when I am reading articles for assignments and I already know what I'm looking for, I realize that it would be very convenient and efficient to have a program that can filter out the information I was looking for. In this program, Line 11 is the main line as it scrapes through the Wikipedia article. Line 12 returns the name/title of the Wikipedia article. Line 15 finds everything with a "p" HTML tag (or paragraph tag) from this Wikipedia article and Line 18 attempts to print the text without the HTML tags. Furthermore, line 24 onwards using an if-else python statement to filter out and spot certain words - in this case, "testing","college", and "project". If those words are found in the article, it will return saying they were found and if not, it will state that none of these words were found in the article. Line 32 onwards filters out any links from this Wikipedia page and presents them under their respective headers.

FUTURE IDEA:
If I were to asked to use this package in my final class project, I would hope to create an Internship searcher that can convert into a CSV file (which I strugged doing for this project). As a third-year student myself who understands the struggles of locating an internship (especially in the pandemic) I feel like it would be beneficial and effecient to find a career-oriented website (like Rutgers Handshake), narrow down an internship subject area (i.e. software engineering) and other parameters like location/date posted/etc, and utilized BeautifulSoup to filter through the multiple available opportunities and parse out the necessary data based on what the user wants. This can then be translated in a CSV or Excel file so it will be easier for the user to view and use.
